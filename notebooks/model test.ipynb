{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/24 18:02:56 WARN Utils: Your hostname, Luo resolves to a loopback address: 127.0.1.1; using 172.29.22.132 instead (on interface eth0)\n",
      "22/09/24 18:02:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/24 18:02:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executer.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/curated/train_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- total_num_consumer: long (nullable = true)\n",
      " |-- avg_dollar_value: double (nullable = true)\n",
      " |-- total_num_transaction: long (nullable = true)\n",
      " |-- mean_income: double (nullable = true)\n",
      " |-- revenue_level: string (nullable = true)\n",
      " |-- total_revenue: double (nullable = true)\n",
      " |-- total_num_postcode: long (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- y_total_num_consumer: long (nullable = true)\n",
      " |-- y_total_revenue: double (nullable = true)\n",
      " |-- y_total_num_transaction: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexed_features = ['revenue_level', 'tag']\n",
    "# We give all values in non-numeric features an index in order to make it ordinal or one-hot encoded\n",
    "indexers =[]\n",
    "for col in indexed_features:\n",
    "  indexers.append(StringIndexer(inputCol=col, outputCol = col+\"_index\"))\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_sdf = pipeline.fit(sdf).transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>total_num_consumer</th><th>avg_dollar_value</th><th>total_num_transaction</th><th>mean_income</th><th>revenue_level</th><th>total_revenue</th><th>total_num_postcode</th><th>tag</th><th>y_total_num_consumer</th><th>y_total_revenue</th><th>y_total_num_transaction</th><th>revenue_level_index</th><th>tag_index</th></tr>\n",
       "<tr><td>10187291046</td><td>87</td><td>111.08408713922158</td><td>87</td><td>61060.0459770115</td><td>b</td><td>31795.597893195016</td><td>87</td><td>watch</td><td>99</td><td>41683.211213258364</td><td>100</td><td>1.0</td><td>9.0</td></tr>\n",
       "<tr><td>10255988167</td><td>218</td><td>389.5552654520502</td><td>218</td><td>63146.619266055044</td><td>b</td><td>366867.5813701302</td><td>211</td><td>computer</td><td>235</td><td>378005.1467314967</td><td>236</td><td>1.0</td><td>0.0</td></tr>\n",
       "<tr><td>10264435225</td><td>1238</td><td>114.10783402533235</td><td>1272</td><td>62006.311320754714</td><td>c</td><td>346896.9592900661</td><td>1018</td><td>watch</td><td>1519</td><td>435003.6795629895</td><td>1566</td><td>2.0</td><td>9.0</td></tr>\n",
       "<tr><td>10364012396</td><td>4</td><td>276.08689369891994</td><td>4</td><td>81123.75</td><td>b</td><td>4008.7818228908673</td><td>4</td><td>music</td><td>16</td><td>19636.790814026932</td><td>16</td><td>1.0</td><td>11.0</td></tr>\n",
       "<tr><td>10385250025</td><td>185</td><td>474.1675409676319</td><td>185</td><td>63441.41081081081</td><td>a</td><td>485974.30939143547</td><td>182</td><td>computer</td><td>190</td><td>494288.86012436583</td><td>191</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>10430380319</td><td>38</td><td>359.1108847340083</td><td>38</td><td>60029.76315789474</td><td>b</td><td>67821.67882777525</td><td>38</td><td>motor</td><td>53</td><td>88318.04454751333</td><td>53</td><td>1.0</td><td>17.0</td></tr>\n",
       "<tr><td>10441711491</td><td>1</td><td>9734.857620793187</td><td>1</td><td>57015.0</td><td>a</td><td>56170.12828629902</td><td>1</td><td>motor</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>17.0</td></tr>\n",
       "<tr><td>10462560289</td><td>405</td><td>37.94568323793217</td><td>413</td><td>64556.179176755446</td><td>c</td><td>46231.12392021321</td><td>366</td><td>gift</td><td>513</td><td>56043.0017308923</td><td>519</td><td>2.0</td><td>4.0</td></tr>\n",
       "<tr><td>10463252268</td><td>22</td><td>464.0964976850653</td><td>22</td><td>60070.77272727273</td><td>a</td><td>67488.91405656068</td><td>22</td><td>artist supply</td><td>26</td><td>78474.65405470507</td><td>26</td><td>0.0</td><td>2.0</td></tr>\n",
       "<tr><td>10530696903</td><td>174</td><td>429.6262128784364</td><td>175</td><td>61263.58857142857</td><td>a</td><td>485692.4365271367</td><td>167</td><td>books</td><td>225</td><td>599049.1086622305</td><td>225</td><td>0.0</td><td>12.0</td></tr>\n",
       "<tr><td>10545955006</td><td>108</td><td>475.27264003873785</td><td>108</td><td>63127.56481481482</td><td>a</td><td>316189.37413271976</td><td>106</td><td>florists</td><td>133</td><td>359500.80479674053</td><td>133</td><td>0.0</td><td>6.0</td></tr>\n",
       "<tr><td>10596295795</td><td>8</td><td>10439.40181102842</td><td>8</td><td>61840.875</td><td>a</td><td>571244.0798428855</td><td>8</td><td>jewelry</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>23.0</td></tr>\n",
       "<tr><td>10618089367</td><td>903</td><td>382.6779937277748</td><td>919</td><td>63214.549510337325</td><td>b</td><td>1410241.1961990686</td><td>783</td><td>stationery</td><td>994</td><td>1427868.1901742313</td><td>1011</td><td>1.0</td><td>14.0</td></tr>\n",
       "<tr><td>10651113986</td><td>17</td><td>537.592173774402</td><td>17</td><td>57981.94117647059</td><td>b</td><td>29701.967601035707</td><td>17</td><td>tent</td><td>24</td><td>40997.390332967654</td><td>24</td><td>1.0</td><td>7.0</td></tr>\n",
       "<tr><td>10702078694</td><td>118</td><td>84.5264917722531</td><td>118</td><td>66654.94067796611</td><td>a</td><td>59346.04797088534</td><td>117</td><td>gift</td><td>126</td><td>75419.01577529583</td><td>127</td><td>0.0</td><td>4.0</td></tr>\n",
       "<tr><td>10739268011</td><td>1</td><td>5275.474679026834</td><td>1</td><td>57933.0</td><td>a</td><td>32971.716743917714</td><td>1</td><td>art dealer</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>21.0</td></tr>\n",
       "<tr><td>10746056613</td><td>67</td><td>75.14601085402924</td><td>67</td><td>65987.23880597015</td><td>a</td><td>34035.13238837724</td><td>67</td><td>music</td><td>72</td><td>35978.902452039714</td><td>72</td><td>0.0</td><td>11.0</td></tr>\n",
       "<tr><td>10854978569</td><td>17</td><td>286.9896314419789</td><td>17</td><td>68877.11764705883</td><td>b</td><td>20637.424490048885</td><td>17</td><td>computer</td><td>28</td><td>32046.023857037744</td><td>28</td><td>1.0</td><td>0.0</td></tr>\n",
       "<tr><td>10901349044</td><td>318</td><td>104.12349108205284</td><td>321</td><td>62609.09034267913</td><td>a</td><td>199204.89947355096</td><td>299</td><td>gift</td><td>390</td><td>254646.5718776761</td><td>391</td><td>0.0</td><td>4.0</td></tr>\n",
       "<tr><td>10922217544</td><td>18</td><td>163.5626661571798</td><td>18</td><td>63804.22222222222</td><td>c</td><td>4946.134870167458</td><td>18</td><td>watch</td><td>19</td><td>5880.61894060871</td><td>19</td><td>2.0</td><td>9.0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------------+------------------+------------------+---------------------+------------------+-------------+------------------+------------------+-------------+--------------------+------------------+-----------------------+-------------------+---------+\n",
       "|merchant_abn|total_num_consumer|  avg_dollar_value|total_num_transaction|       mean_income|revenue_level|     total_revenue|total_num_postcode|          tag|y_total_num_consumer|   y_total_revenue|y_total_num_transaction|revenue_level_index|tag_index|\n",
       "+------------+------------------+------------------+---------------------+------------------+-------------+------------------+------------------+-------------+--------------------+------------------+-----------------------+-------------------+---------+\n",
       "| 10187291046|                87|111.08408713922158|                   87|  61060.0459770115|            b|31795.597893195016|                87|        watch|                  99|41683.211213258364|                    100|                1.0|      9.0|\n",
       "| 10255988167|               218| 389.5552654520502|                  218|63146.619266055044|            b| 366867.5813701302|               211|     computer|                 235| 378005.1467314967|                    236|                1.0|      0.0|\n",
       "| 10264435225|              1238|114.10783402533235|                 1272|62006.311320754714|            c| 346896.9592900661|              1018|        watch|                1519| 435003.6795629895|                   1566|                2.0|      9.0|\n",
       "| 10364012396|                 4|276.08689369891994|                    4|          81123.75|            b|4008.7818228908673|                 4|        music|                  16|19636.790814026932|                     16|                1.0|     11.0|\n",
       "| 10385250025|               185| 474.1675409676319|                  185| 63441.41081081081|            a|485974.30939143547|               182|     computer|                 190|494288.86012436583|                    191|                0.0|      0.0|\n",
       "| 10430380319|                38| 359.1108847340083|                   38| 60029.76315789474|            b| 67821.67882777525|                38|        motor|                  53| 88318.04454751333|                     53|                1.0|     17.0|\n",
       "| 10441711491|                 1| 9734.857620793187|                    1|           57015.0|            a| 56170.12828629902|                 1|        motor|                null|              null|                   null|                0.0|     17.0|\n",
       "| 10462560289|               405| 37.94568323793217|                  413|64556.179176755446|            c| 46231.12392021321|               366|         gift|                 513|  56043.0017308923|                    519|                2.0|      4.0|\n",
       "| 10463252268|                22| 464.0964976850653|                   22| 60070.77272727273|            a| 67488.91405656068|                22|artist supply|                  26| 78474.65405470507|                     26|                0.0|      2.0|\n",
       "| 10530696903|               174| 429.6262128784364|                  175| 61263.58857142857|            a| 485692.4365271367|               167|        books|                 225| 599049.1086622305|                    225|                0.0|     12.0|\n",
       "| 10545955006|               108|475.27264003873785|                  108| 63127.56481481482|            a|316189.37413271976|               106|     florists|                 133|359500.80479674053|                    133|                0.0|      6.0|\n",
       "| 10596295795|                 8| 10439.40181102842|                    8|         61840.875|            a| 571244.0798428855|                 8|      jewelry|                null|              null|                   null|                0.0|     23.0|\n",
       "| 10618089367|               903| 382.6779937277748|                  919|63214.549510337325|            b|1410241.1961990686|               783|   stationery|                 994|1427868.1901742313|                   1011|                1.0|     14.0|\n",
       "| 10651113986|                17|  537.592173774402|                   17| 57981.94117647059|            b|29701.967601035707|                17|         tent|                  24|40997.390332967654|                     24|                1.0|      7.0|\n",
       "| 10702078694|               118|  84.5264917722531|                  118| 66654.94067796611|            a| 59346.04797088534|               117|         gift|                 126| 75419.01577529583|                    127|                0.0|      4.0|\n",
       "| 10739268011|                 1| 5275.474679026834|                    1|           57933.0|            a|32971.716743917714|                 1|   art dealer|                null|              null|                   null|                0.0|     21.0|\n",
       "| 10746056613|                67| 75.14601085402924|                   67| 65987.23880597015|            a| 34035.13238837724|                67|        music|                  72|35978.902452039714|                     72|                0.0|     11.0|\n",
       "| 10854978569|                17| 286.9896314419789|                   17| 68877.11764705883|            b|20637.424490048885|                17|     computer|                  28|32046.023857037744|                     28|                1.0|      0.0|\n",
       "| 10901349044|               318|104.12349108205284|                  321| 62609.09034267913|            a|199204.89947355096|               299|         gift|                 390| 254646.5718776761|                    391|                0.0|      4.0|\n",
       "| 10922217544|                18| 163.5626661571798|                   18| 63804.22222222222|            c| 4946.134870167458|                18|        watch|                  19|  5880.61894060871|                     19|                2.0|      9.0|\n",
       "+------------+------------------+------------------+---------------------+------------------+-------------+------------------+------------------+-------------+--------------------+------------------+-----------------------+-------------------+---------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features =  [\"tag_index\", \"revenue_level_index\"]\n",
    "# one-hot-encoding the numeric indices\n",
    "ohe = []\n",
    "for f in categorical_features:\n",
    "  ohe.append(OneHotEncoder(inputCol=f, outputCol=f+\"OHE\"))\n",
    "\n",
    "pipeline = Pipeline(stages=ohe)\n",
    "encoded_sdf = pipeline.fit(indexed_sdf).transform(indexed_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- total_num_consumer: long (nullable = true)\n",
      " |-- avg_dollar_value: double (nullable = true)\n",
      " |-- total_num_transaction: long (nullable = true)\n",
      " |-- mean_income: double (nullable = true)\n",
      " |-- revenue_level: string (nullable = true)\n",
      " |-- total_revenue: double (nullable = true)\n",
      " |-- total_num_postcode: long (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- y_total_num_consumer: long (nullable = true)\n",
      " |-- y_total_revenue: double (nullable = true)\n",
      " |-- y_total_num_transaction: long (nullable = true)\n",
      " |-- revenue_level_index: double (nullable = false)\n",
      " |-- tag_index: double (nullable = false)\n",
      " |-- tag_indexOHE: vector (nullable = true)\n",
      " |-- revenue_level_indexOHE: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['total_num_consumer', 'avg_dollar_value', 'total_num_transaction', 'mean_income', 'revenue_level_indexOHE', \\\n",
    "    'total_revenue', 'total_num_postcode', 'tag_indexOHE']\n",
    "assembler = VectorAssembler(inputCols=features ,outputCol='features')\n",
    "vectorised_training_set = assembler.transform(encoded_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorised_training_set = vectorised_training_set.select('features', 'y_total_revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>features</th><th>y_total_revenue</th></tr>\n",
       "<tr><td>(33,[0,1,2,3,5,8,...</td><td>41683.211213258364</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,5,8,...</td><td>378005.1467314967</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,6,8,...</td><td>435003.6795629895</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,5,8,...</td><td>19636.790814026932</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>494288.86012436583</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,5,8,...</td><td>88318.04454751333</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>null</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,6,8,...</td><td>56043.0017308923</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>78474.65405470507</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>599049.1086622305</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>359500.80479674053</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>null</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,5,8,...</td><td>1427868.1901742313</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,5,8,...</td><td>40997.390332967654</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>75419.01577529583</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>null</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>35978.902452039714</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,5,8,...</td><td>32046.023857037744</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,4,8,...</td><td>254646.5718776761</td></tr>\n",
       "<tr><td>(33,[0,1,2,3,6,8,...</td><td>5880.61894060871</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+------------------+\n",
       "|            features|   y_total_revenue|\n",
       "+--------------------+------------------+\n",
       "|(33,[0,1,2,3,5,8,...|41683.211213258364|\n",
       "|(33,[0,1,2,3,5,8,...| 378005.1467314967|\n",
       "|(33,[0,1,2,3,6,8,...| 435003.6795629895|\n",
       "|(33,[0,1,2,3,5,8,...|19636.790814026932|\n",
       "|(33,[0,1,2,3,4,8,...|494288.86012436583|\n",
       "|(33,[0,1,2,3,5,8,...| 88318.04454751333|\n",
       "|(33,[0,1,2,3,4,8,...|              null|\n",
       "|(33,[0,1,2,3,6,8,...|  56043.0017308923|\n",
       "|(33,[0,1,2,3,4,8,...| 78474.65405470507|\n",
       "|(33,[0,1,2,3,4,8,...| 599049.1086622305|\n",
       "|(33,[0,1,2,3,4,8,...|359500.80479674053|\n",
       "|(33,[0,1,2,3,4,8,...|              null|\n",
       "|(33,[0,1,2,3,5,8,...|1427868.1901742313|\n",
       "|(33,[0,1,2,3,5,8,...|40997.390332967654|\n",
       "|(33,[0,1,2,3,4,8,...| 75419.01577529583|\n",
       "|(33,[0,1,2,3,4,8,...|              null|\n",
       "|(33,[0,1,2,3,4,8,...|35978.902452039714|\n",
       "|(33,[0,1,2,3,5,8,...|32046.023857037744|\n",
       "|(33,[0,1,2,3,4,8,...| 254646.5718776761|\n",
       "|(33,[0,1,2,3,6,8,...|  5880.61894060871|\n",
       "+--------------------+------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorised_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = vectorised_training_set.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2751, 1202)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/24 22:32:47 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 6344078 ms exceeds timeout 120000 ms\n",
      "22/09/24 22:32:47 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol='y_total_revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/stluo/ADS tut/generic-buy-now-pay-later-project-group-14/notebooks/model test.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/stluo/ADS%20tut/generic-buy-now-pay-later-project-group-14/notebooks/model%20test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train\u001b[39m.\u001b[39;49mselect(F\u001b[39m.\u001b[39;49mcol(\u001b[39m'\u001b[39;49m\u001b[39my_total_revenue\u001b[39;49m\u001b[39m'\u001b[39;49m))[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "train.where(F.col('y_total_revenue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/24 22:41:03 ERROR Executor: Exception in task 1.0 in stage 23.0 (TID 24)\n",
      "scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[12.0,18309.168774046513,12.0,58310.25,1.0,1333639.8912172737,12.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/09/24 22:41:03 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 23)\n",
      "scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[8.0,10439.40181102842,8.0,61840.875,1.0,571244.0798428855,8.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/09/24 22:41:03 WARN TaskSetManager: Lost task 1.0 in stage 23.0 (TID 24) (172.29.22.132 executor driver): scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[12.0,18309.168774046513,12.0,58310.25,1.0,1333639.8912172737,12.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/09/24 22:41:03 ERROR TaskSetManager: Task 1 in stage 23.0 failed 1 times; aborting job\n",
      "22/09/24 22:41:03 WARN TaskSetManager: Lost task 0.0 in stage 23.0 (TID 23) (172.29.22.132 executor driver): scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[8.0,10439.40181102842,8.0,61840.875,1.0,571244.0798428855,8.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/09/24 22:41:03 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 23.0 failed 1 times, most recent failure: Lost task 1.0 in stage 23.0 (TID 24) (172.29.22.132 executor driver): scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[12.0,18309.168774046513,12.0,58310.25,1.0,1333639.8912172737,12.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n",
      "\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:512)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:496)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[12.0,18309.168774046513,12.0,58310.25,1.0,1333639.8912172737,12.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o287.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 23.0 failed 1 times, most recent failure: Lost task 1.0 in stage 23.0 (TID 24) (172.29.22.132 executor driver): scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[12.0,18309.168774046513,12.0,58310.25,1.0,1333639.8912172737,12.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:512)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:496)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[12.0,18309.168774046513,12.0,58310.25,1.0,1333639.8912172737,12.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/stluo/ADS tut/generic-buy-now-pay-later-project-group-14/notebooks/model test.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/stluo/ADS%20tut/generic-buy-now-pay-later-project-group-14/notebooks/model%20test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m fitted_model \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39;49mfit(train)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 379\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    380\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o287.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 23.0 failed 1 times, most recent failure: Lost task 1.0 in stage 23.0 (TID 24) (172.29.22.132 executor driver): scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[12.0,18309.168774046513,12.0,58310.25,1.0,1333639.8912172737,12.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:512)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:496)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: scala.MatchError: [null,1.0,(33,[0,1,2,3,4,8,9],[12.0,18309.168774046513,12.0,58310.25,1.0,1333639.8912172737,12.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "fitted_model = lr.fit(train)\n",
    "#fitted_model.setFeaturesCol(\"features\")\n",
    "#fitted_model.setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
