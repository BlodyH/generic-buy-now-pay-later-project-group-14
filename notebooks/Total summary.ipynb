{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buy Now, Pay Later Project\n",
    "\n",
    "The Buy Now, Pay Later (BNPL) Firm has begun offering a new “Pay in 5 Installments” feature and is going to onboard 100 merchants every year. This project focuses on these tasks:\n",
    "\n",
    "\n",
    "- Overview of consumer and transaction data\n",
    "- Analysis to find the 100 best merchants\n",
    "- Recommendations for BNPL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided for this project includes:\n",
    "- Transaction Dataset\n",
    "- Consumer Dataset\n",
    "- Merchant Dataset\n",
    "\n",
    "External Dataset employed to provide more insights into the consumer analysis:\n",
    "- [Australian postcode](https://www.matthewproctor.com/australian_postcodes)\n",
    "- Income by SA2 Districts（ABS）\n",
    "- SA2 shapefile (ABS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 23:33:44 WARN Utils: Your hostname, Luo resolves to a loopback address: 127.0.1.1; using 172.20.233.63 instead (on interface eth0)\n",
      "22/10/07 23:33:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 23:33:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction dataset includes 14195505 transaction records.\n",
      "Features included are: \n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_sdf = spark.read.parquet('../data/tables/transactions_*/*')\n",
    "print(f\"Transaction dataset includes {transaction_sdf.count()} transaction records.\")\n",
    "print(\"Features included are: \")\n",
    "transaction_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer dataset includes 499999 consumer records.\n",
      "Features included are: \n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- consumer_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consumer_sdf = spark.read.option(\"delimiter\", \"|\").csv('../data/tables/tbl_consumer.csv', inferSchema =True, header=True)\n",
    "print(f\"Consumer dataset includes {consumer_sdf.count()} consumer records.\")\n",
    "print(\"Features included are: \")\n",
    "consumer_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant dataset includes 4026 merchant records.\n",
      "Features included are: \n",
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- revenue_level: string (nullable = true)\n",
      " |-- take_rate: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merchant_sdf = spark.read.csv(\"../data/curated/merchant.csv\", inferSchema =True, header=True)\n",
    "print(f\"Merchant dataset includes {merchant_sdf.count()} merchant records.\")\n",
    "print(\"Features included are: \")\n",
    "merchant_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External Datasets Overview\n",
    "\n",
    "1. **Australian Postcode**  \n",
    "Used to convert postcode of each region to their SA2 code for furture geospatial plotting\n",
    "\n",
    "2. **Income by SA2 Districts**  \n",
    "Used to analyse the purchase power of consumers from different regions which may correlate with final assessment of the merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features included are: \n",
      "root\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- SA2_code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postcode_SA2_sdf = spark.read.csv(\"../data/curated/processed_postcode.csv\", inferSchema =True, header=True)\n",
    "print(\"Features included are: \")\n",
    "postcode_SA2_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features included are: \n",
      "root\n",
      " |-- SA2_code: string (nullable = true)\n",
      " |-- mean_total_income: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "income_sdf = spark.read.csv(\"../data/curated/processed_income.csv\", inferSchema =True, header=True)\n",
    "print(\"Features included are: \")\n",
    "income_sdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial Visualisation\n",
    "\n",
    "We inspect the relationship between each of the three features with respect to the location:\n",
    "- Mean total income\n",
    "- Number of Consumers\n",
    "- Number of Transactions\n",
    "\n",
    "这里有分析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../plots/mean%20total%20income%20geo.png\" width=\"500\"/> \n",
    "<img src=\"../plots/large%20mean%20total%20income%20geo.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../plots/num%20consumers.png\" width=\"500\"/>\n",
    "<img src=\"../plots/num%20transactions.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
