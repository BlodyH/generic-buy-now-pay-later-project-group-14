{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Model\n",
    "This notebook:\n",
    "1. demonstrates the distribution of two fraud probabilities\n",
    "2. converts categorical data into continuous data\n",
    "3. splits transactions with given fraud probabilities into training and validation set\n",
    "4. fits a logistic regression model on training set and evaluates on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executer.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "Examine the distribution of both delta files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read delta files\n",
    "merchant_df = pd.read_csv(\"../data/tables/merchant_fraud_probability.csv\")\n",
    "consumer_df = pd.read_csv(\"../data/tables/consumer_fraud_probability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot of consumer fraud probability\n",
    "sns.histplot(consumer_df[\"fraud_probability\"], bins=50)\n",
    "plt.title(\"Distribution of Consumer Fraud Probability\", fontsize=15)\n",
    "plt.xlabel(\"Fraud Probability\", fontsize=13)\n",
    "plt.ylabel(\"Count\", fontsize=13)\n",
    "plt.savefig(\"../plots/Distribution of Consumer Fraud Probability.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot of merchant fraud probability\n",
    "sns.histplot(merchant_df[\"fraud_probability\"], bins=30)\n",
    "plt.title(\"Distribution of Merchant Fraud Probability\", fontsize=15)\n",
    "plt.xlabel(\"Fraud Probability\", fontsize=13)\n",
    "plt.ylabel(\"Count\", fontsize=13)\n",
    "plt.savefig(\"../plots/Distribution of Merchant Fraud Probability.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = spark.read.parquet(\"../data/curated/full_data/\")\n",
    "probs_merchant = spark.read.option('header', True).csv('../data/tables/merchant_fraud_probability.csv')\n",
    "probs_consumer= spark.read.option('header', True).csv('../data/tables/consumer_fraud_probability.csv')\n",
    "full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features to appropriate data types\n",
    "probs_consumer =  probs_consumer.withColumn('user_id', F.col('user_id').cast('long'))\\\n",
    "                                .withColumn('fraud_probability', F.col('fraud_probability').cast('float'))\n",
    "probs_merchant =  probs_merchant.withColumn('merchant_abn', F.col('merchant_abn').cast('long'))\\\n",
    "                                .withColumn('fraud_probability', F.col('fraud_probability').cast('float'))\n",
    "\n",
    "# merge transaction file with merchants'/consumers' fraud probability based on merchant abn or user id respectively by left join\n",
    "full = full.join(probs_merchant, on = ['merchant_abn', 'order_datetime'], how = 'left').withColumnRenamed('fraud_probability', 'merchant_prob')\n",
    "full = full.join(probs_consumer, on = ['user_id', 'order_datetime'], how = 'left').withColumnRenamed('fraud_probability', 'consumer_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all the missing value with 0.01 as default fraud prob\n",
    "full = full.na.fill(value=0.01, subset=['merchant_prob', 'consumer_prob'])\n",
    "\n",
    "# set benchmark as 5% to focus on False Positive instead of False Negative\n",
    "full = full.withColumn('is_fraud', F.when((F.col('merchant_prob') > 5) | (F.col('consumer_prob') > 5), 1).otherwise(0))\n",
    "\n",
    "# discard extremely small values\n",
    "full = full.filter(F.col('dollar_value') >= 1).na.drop(subset = 'name')\n",
    "full =  full.withColumn('month', F.month('order_datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In {} transactions, {} are detected as fraud'.format(full.count(), full.filter(F.col('is_fraud') == 1).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index ordinal features & one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full =  full.withColumn('month', F.month('order_datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_features = ['revenue_level', 'tags', 'gender']\n",
    "indexers =[]\n",
    "for col in indexed_features:\n",
    "    indexers.append(StringIndexer(inputCol=col, outputCol = col+\"_index\"))\n",
    "\n",
    "# one-hot encode the numeric indices\n",
    "categorical_features =  [\"tags_index\", \"gender_index\",\"month\"]\n",
    "ohe = []\n",
    "for f in categorical_features:\n",
    "    ohe.append(OneHotEncoder(inputCol=f, outputCol=f+\"OHE\"))\n",
    "\n",
    "pipeline = Pipeline(stages=indexers+ohe)\n",
    "indexed_result = pipeline.fit(full).transform(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selected = ['dollar_value','take_rate','mean_total_income','monthOHE','revenue_level_index','tags_indexOHE','gender_indexOHE']\n",
    "assembler = VectorAssembler(inputCols=feature_selected ,outputCol='features')\n",
    "processed_data = assembler.transform(indexed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced distribution of two classes. We decide to split the data according to their class and make the distribution balanced\n",
    "fraud_data = processed_data.filter(F.col('is_fraud') == 1)\n",
    "normal_data = processed_data.filter(F.col('is_fraud') == 0).randomSplit([0.01,0.99], 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train_fraud,test_fraud = fraud_data.randomSplit([0.7,0.3])\n",
    "train_normal,test_normal = normal_data.randomSplit([0.7,0.3])\n",
    "train_data = train_fraud.union(train_normal).orderBy(rand())\n",
    "test_data = test_fraud.union(test_normal).orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.count(),test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol='is_fraud')\n",
    "fitted_model = lr.fit(train_data)\n",
    "fitted_model.setFeaturesCol(\"features\")\n",
    "fitted_model.setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance\n",
    "pred_and_labels = fitted_model.evaluate(test_data)\n",
    "score_and_label = pred_and_labels.predictions.select('prediction', 'is_fraud').withColumnRenamed('is_fraud', 'label')\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display evaluation metrics\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(score_and_label)))\n",
    "tp = score_and_label.filter((F.col('prediction') == 1) & (F.col('label') == 1)).count()\n",
    "fn = score_and_label.filter((F.col('prediction') == 0) & (F.col('label') == 1)).count()\n",
    "recall = tp/(tp+fn)\n",
    "print('recall: ' + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "703e593df40508a60fa363339ca2bbb5bae045b0a530fb0e89bc3e7c255f1da9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
